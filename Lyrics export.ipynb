{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T15:33:01.909212Z",
     "start_time": "2019-09-03T15:33:01.904901Z"
    }
   },
   "source": [
    "# Project 5: Alternative R&B for Me\n",
    "Purpose of project: Create a 2 pronged recommendation system for Alternative R&B songs. You can get songs based on similarities in lyrics or on similar audio sounds.\n",
    "\n",
    "**Packages used:**\n",
    "+ Keras\n",
    "+ Gensim\n",
    "+ pymongo\n",
    "+ Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T16:20:48.212661Z",
     "start_time": "2019-09-18T16:20:48.204587Z"
    }
   },
   "outputs": [],
   "source": [
    "import lyricsgenius\n",
    "from Dans_Genius_API import my_api_token #personal token, go toe Genius.com to get your own!\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from ast import literal_eval\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Alternative R&B Artists\n",
    "Created by cross referencing 2 lists found onlines as well as adding/removing artist based on domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T04:15:34.168957Z",
     "start_time": "2019-09-06T04:15:34.155816Z"
    }
   },
   "outputs": [],
   "source": [
    "rnb_artists = [\n",
    "\"11:11\", \"Abra\", \"Active Child\", \"Alessia Cara\", \"Alex Clare\", \"Allan Kingdom\", \"Aloe Blacc\", \"AlunaGeorge\",\n",
    "\"Always Never\", \"Amber Coffman\", \"Anders\", \"Anderson Paak\", \"Anna Wise\", \"Ari Lennox\", \"Arlissa\", \"Autre Ne Veut\",\n",
    "\"Banks\", \"Black Atlass\", \"Blackbear\", \"Blood Orange\", \"Boots\", \"Bryson Tiller\", \"Chet Faker\", \"Childish Gambino\",\n",
    "\"Clarence Clarity\", \"Cocaine 80s\", \"D'Angelo\", \"Daley\", \"Daniel Caesar\", \"Danny!\", \"Dawn Richard\", \"Dean\",\n",
    "\"Dvsn\", \"Elijah Blake\", \"Erykah Badu\", \"Estelle\", \"FKA twigs\", \"Francis and the Lights\", \"Frank Ocean\",\n",
    "\"Gallant\", \"GoldLink\", \"Grimes\", \"Groove Theory\", \"H.E.R.\", \"Hiatus Kaiyote\", \"How To Dress Well\", \"Ibeyi\",\n",
    "\"Illangelo\", \"ILoveMakonnen\", \"Inc.\", \"Jack Garratt\", \"Jai Paul\", \"James Fauntleroy\", \"Jamie Woon\", \"Jamila Woods\",\n",
    "\"Janelle Monáe\", \"Jesse Boykins III\", \"Jessy Lanza\", \"Jhené Aiko\", \"JMSN\", \"Jon Bellion\", \"Jorja Smith\", \"Kacy Hill\",\n",
    "\"Kali Uchis\", \"Kaytranada\", \"Kehlani\", \"Kelela\", \"Kelis\", \"Kenna\", \"Kevin Abstract\", \"Khalid\", \"Kiana Ledé\",\n",
    "\"Kid Cudi\", \"Kiiara\", \"Kilo Kish\", \"Kimbra\", \"King\", \"Lance Skiiiwalker\", \"Lapalux\", \"Låpsley\", \"Lauv\",\n",
    "\"Lion Babe\", \"Little Dragon\", \"Lykke Li\", \"M.I.A.\", \"Mabel\", \"Mac Ayres\", \"Maejor\", \"Mahalia\", \"Majid Jordan\",\n",
    "\"Malay\", \"Marian Hill\", \"Mateo\", \"Matt Martians\", \"Maxwell\", \"Miguel\", \"Mila J\", \"Mr Hudson\", \"Nao\",\n",
    "\"Nick Murphy\", \"NxWorries\", \"Oh Wonder\", \"PARTYNEXTDOOR\", \"Pell\", \"Perfume Genius\", \"Quadron\",\"R.LUM.R\",\n",
    "\"Rainy Milo\", \"Raleigh Ritchie\", \"Raury\", \"Reggie Sears\", \"Rhye\", \"River Tiber\", \"Ro James\", \"Rosie Lowe\",\n",
    "\"Roy Woods\", \"Sabrina Claudio\", \"Samantha Urbani\", \"Sampha\", \"Seinabo Sey\", \"Sevdaliza\", \"Sevyn Streeter\",\n",
    "\"Shura\", \"Shy Girls\", \"Sia Furler\", \"Sinéad Harnett\", \"Snakehips\", \"SOHN\", \"Solange\", \"Spooky Black\",\n",
    "\"Steve Lacy\", \"Syd tha Kid\", \"SZA\", \"Tei Shi\", \"The Internet\", \"The Neighbourhood\", \"The Weeknd\", \"Thee Satisfaction\",\n",
    "\"THEY.\", \"Thundercat\", \"Tinashe\", \"Toro y Moi\", \"Tory Lanez\", \"Travis Scott\", \"Wet\", \"William Singe\",\n",
    "\"Willow Smith\", \"Yummy Bingham\", \"Yuna\", \"Zayn\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying data from MongoDB to create a dataframe\n",
    "**NOTE: This has to be done after running the Song_lyric_scrape.py file and successfully storing data in your mongoDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T15:08:40.146908Z",
     "start_time": "2019-09-04T15:08:39.695963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10105\n",
      "Shape before dropping the dups:  (10105, 4)\n",
      "Shape after dropping the dups:  (10093, 4)\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient()\n",
    "db = client.sep_19_songs #create connection with database\n",
    "\n",
    "df = pd.DataFrame(list(db.sep_19_songs.find({}, {'_id':0}))) #extract all except mongo auto-generated id field\n",
    "print(\"Shape before dropping the dups: \", df.shape)\n",
    "df.drop_duplicates(inplace=True) ###remove duplicates incase you had to run song_lyric_scrape multiple times..\n",
    "print(\"Shape after dropping the dups: \", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECKPOINT: Save/Load dataframe created from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T06:48:55.807880Z",
     "start_time": "2019-09-13T06:48:55.732681Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "#save file: unccoment to do so\n",
    "with open('orig_df.pkl', 'wb') as picklefile:\n",
    "   pickle.dump(df, picklefile)    \n",
    "'''\n",
    "#\n",
    "'''\n",
    "#load file: uncomment to do so\n",
    "with open('orig_df.pkl', 'rb') as picklefile:\n",
    "    df = pickle.load(picklefile)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T06:48:57.118589Z",
     "start_time": "2019-09-13T06:48:57.112720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of df (10093, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of df\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting Artist spelling (due to unicode characters)/ Removing out-of-genre artists\n",
    "Removing artists that shouldnt be included in the modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T06:48:59.572798Z",
     "start_time": "2019-09-13T06:48:59.557987Z"
    }
   },
   "outputs": [],
   "source": [
    "#dict to change names to proper names w/o unicode/ change special letters to regular letters\n",
    "name_mapper = {\n",
    " '\\u200banders': 'anders',\n",
    " '\\u200bblackbear': 'blackbear',\n",
    " '\\u200bdvsn': 'dvsn',\n",
    " '\\u200biLoveMakonnen': 'iLoveMakonnen',\n",
    " '\\u200b¿\\u200bT\\u200be\\u200bo\\u200b?\\u200b': 'Teo',\n",
    " 'Sinéad Harnett': 'Sinead Harnett',\n",
    " 'Jhené Aiko': 'Jhene Aiko',\n",
    " 'Kiana Ledé': 'Kiana Lede'\n",
    "}\n",
    "\n",
    "df['Artist_name'] = df['Artist_name'].replace(name_mapper) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T06:49:00.703381Z",
     "start_time": "2019-09-13T06:49:00.691280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before removing artists (10093, 4)\n",
      "shape after removing artists (9639, 4)\n"
     ]
    }
   ],
   "source": [
    "#remove some artists that dont fit the genre/ were grabbed with fuzzy matching from API\n",
    "wrong_artists = ['iLoveMakonnen','6ix9ine', 'Alex Clare', 'Allan Kingdom','D’Angelo','Francis and the Lights']\n",
    "\n",
    "print(\"shape before removing artists\", df.shape)\n",
    "df = df[~df[\"Artist_name\"].isin(wrong_artists)].reset_index(drop=True)\n",
    "print(\"shape after removing artists\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing duplicate songs and alt-versions/remixes/skits/etc\n",
    "The Genius API returned quite abit of duplicate songs that need to be filtered out. These include:\n",
    "+ Acoustic\n",
    "+ Remixes\n",
    "+ Skits\n",
    "+ Demos/Previews\n",
    "+ etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T06:49:08.348603Z",
     "start_time": "2019-09-13T06:49:07.266077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before removing dup songs (9639, 4)\n",
      "shape after removing dup songs (8216, 4)\n"
     ]
    }
   ],
   "source": [
    "#General list types of songs to remove from the dataframe\n",
    "gen_dup_songs = ['freestyle',\n",
    "'remix', '\\(live', '- live', 'acoustic', 'skit', 'no lyrics yet!', 'version', 'türkçe', 'turkce', 'demo\\)', \n",
    "             'mix\\)', 'radio edit', 'remaster', 'cover\\)', '\\(unreleased', '\\(single\\)', '\\(snippet\\)', \n",
    "             '\\(spotify session\\)', '\\(spotify singles\\)', 'session', 'without justin bieber', 'trailer',\n",
    "             'en español\\)', 'edit\\)','re-work', 'rework\\)', 'bonus track', 'accapeela', 'acapella\\)',\n",
    "             '- acapella', 'remake\\)', 'godspeed screenplay', 'mix\\]', 'cover\\]', \n",
    "             '\\(teaser', '\\[demo', 'leak', 'short film','bbc radio', 'reimagined', 'tedx talk', 'music video',\n",
    "             'lollapalooza', '\\(unfinished\\)', 'radio', 'reprise\\)','mixture', '\\[duplicate\\]', 'medley',\n",
    "             'tba\\*','mashup', 'freshman', 'stripped', 'refix', 'dub', 'uncut', '- cut', 'edit', 'speech', '\\@',\n",
    "              'untitled', 'interlude', 'dates' ]\n",
    "\n",
    "#specific cases for this project/ songs extracted\n",
    "specific_cases= ['talk \\(disclosure vip\\)', 'intro - \\(let’s talk about it\\)', \n",
    "                 'travis scott takes over hot 97 in the am', 'khalid - better \\(official music video\\)',\n",
    "              'Home Going - 2354122', 'cliche r&b joint with auto tune \\(i do\\)', \n",
    "             'waiting game \\(kaytranada edition\\)', 'love & feeling \\(sleep d dub\\)',\n",
    "             'because the internet', 'both hands \\(black rainbow\\)', 'clapping for the wrong reasons',\n",
    "             'childish gambino @ the atrium', 'complex party house freestyle',\n",
    "             'complex photo shoot', 'drexel university performance', 'i love clothes \\(deadbeat summer\\)',\n",
    "             'leaving one direction', 'song notes', 'be on my \\(interlude\\)', 'going \\(interlude\\)',\n",
    "             'lady luck - royce wood jr retwix', 'betty \\(for boogie\\)', 'locked inside -walsh', 'jhene aiko’s tattoos',\n",
    "             'my name is jhene', '4th of july \\(fireworks\\)', 'milkshake 2', 'segue', 'drake diss', 'twitter note',\n",
    "             'fire fire \\(piracy funds terrorism\\)', 'the p is mine', 'voice memo', 'chppd', 'breakdown\\)', \n",
    "             'open letter to fans', 'royalty', 'false skull 7', 'bitches talk \\(repeat\\)', 'show respect', \n",
    "             'girl with the tattoo enter.lewd', \"coachella interlude\", '102 hours of introductions', 'sampler',\n",
    "             'tumblr post on adjectives', 'the deep web tour dates', 'not on doasm 03', 'copernicus landing',\n",
    "             'warm up \\(cloud 9\\)', 'tathagātagarbha', 'response to grammy awards producers', 'beltway', 'ibeyi',\n",
    "             'beltway',\n",
    "                ]\n",
    "\n",
    "#combine the lists together\n",
    "dup_songs =  gen_dup_songs + specific_cases\n",
    "\n",
    "\n",
    "print(\"shape before removing dup songs\", df.shape)\n",
    "for song_remove in dup_songs:\n",
    "    df = df[~df['Song'].str.lower().str.contains(song_remove)]\n",
    "print(\"shape after removing dup songs\", df.shape)\n",
    "#replace weird unicode in song titles\n",
    "df['Song'] = df['Song'].str.replace('\\u200b', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T17:21:03.260472Z",
     "start_time": "2019-09-06T17:21:03.256163Z"
    }
   },
   "source": [
    "### Remove songs that do not have lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T06:49:13.072347Z",
     "start_time": "2019-09-13T06:49:13.067663Z"
    }
   },
   "outputs": [],
   "source": [
    "#returns of lyrics from songs that dont have any lyrics listed in the API\n",
    "bad_lyrics = ['lyrics for this song have yet to be released',\n",
    "              'no lyrics yet!', '\\[instrumental\\]', '\\[spoken interlude\\]', '\\(instrumental\\)',\n",
    "              '\\(instrumental with vocals\\)', '\\[instrumental w/ vocalisations\\]',\n",
    "              'lyrics will be available upon release.' 'lyrics are yet to be released',\n",
    "              'stay tuned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T06:49:16.924405Z",
     "start_time": "2019-09-13T06:49:16.461173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before removing bad lyrics (8216, 4)\n",
      "shape after removing bad lyrics (7421, 4)\n"
     ]
    }
   ],
   "source": [
    "#remove tagged bad lyrics\n",
    "print(\"shape before removing bad lyrics\", df.shape)\n",
    "for song_remove in bad_lyrics: #remove songs that have any of the above \"lyrics\"\n",
    "    df = df[~(df['Lyrics'].str.lower().str.contains(song_remove, na=False))]\n",
    "    \n",
    "df = df[df['Lyrics']!=\"\"] #removes those with empty lyrics\n",
    "df =df[df['Lyrics'].str.len() > 300] #removed songs w/ less than 300 characters (really short songs skew results)\n",
    "print(\"shape after removing bad lyrics\", df.shape)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning text of lyrics\n",
    "Processing the data and performing the following steps:\n",
    "+ Getting rid of song structure tags (ie [chorus])\n",
    "+ Replace '\\n' escape character from lyrics\n",
    "+ Lemmatize words to better aid doc2vec model\n",
    "+ Remove punctuation and transform to lowercase\n",
    "+ Replace double spaces with single spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T06:49:24.144729Z",
     "start_time": "2019-09-13T06:49:21.664605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7421, 4)\n"
     ]
    }
   ],
   "source": [
    "##Cleaning up additional stuff in the lyrics\n",
    "df['Lyrics'] = df['Lyrics'].str.replace(\"[\\[].*?[\\]]\", \"\") #1: gets rid of verse/chorus in brackets\n",
    "df['Lyrics'] = df['Lyrics'].str.replace(\"\\n\", \" \").str.lower() #2: replace '\\n' with space + lower\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['Lyrics'] = df['Lyrics'].apply(lambda x : lemmatizer.lemmatize(str(x))) #3: lemmatize words before removing punc\n",
    "\n",
    "#alphanumeric = (lambda x: re.sub('\\w*\\d\\w*', ' ', x)) #remove non alpha numeric values/ OFFF\n",
    "punc_lower = (lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', str(x))) #get rid of punctuation+lower\n",
    "df['Lyrics'] = df['Lyrics'].map(punc_lower)  #4: applying above to remove punc\n",
    "\n",
    "df['Lyrics'] = df['Lyrics'].str.replace(\"  \", \" \") #5: replace double spaces (do after replace punc)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T22:19:38.773669Z",
     "start_time": "2019-09-08T22:19:38.766760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' i love to fly it s just you re alone with peace and quiet nothing around you but clear blue sky no one to hassle you no one to tell you where to go or what to do the only bad part about flying is having to come back down to the fucking world  i wasn t mistreated  he whispered as he came and now you re just sailing on you re sending on your pain i was undressed in all your shame you re sailing waters too deep for me to care have you ever wondered why you stress so hard you can t even seem to wonder what s on your mind have you ever held yourself on a secret all in there have you ever had yourself for all one time have you ever asked you why are you cheapening yourself  have you ever let a look of goodness spread across your face have you ever loved yourself out of a secret all in there say my name or say whatever i was in the streets when what s his name  came sailing on you re resting on your name how was i to rest under all your weight you say my love was undressed from all your strength and we sail away and we sail away and we sail away and we sail away but i was left alone i just one left alone'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking results of cleaning > looks great\n",
    "df.iloc[2351].Lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECKPOINT: Saving cleaned dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrical Recommendation - Using Doc2vec\n",
    "Steps taken:\n",
    "+ Create a class used to tokentize and create TaggedDocuments where tags are \"Artist|Song\"\n",
    "+ Create and train Doc2Vec model\n",
    "+ Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T17:05:33.414677Z",
     "start_time": "2019-09-16T17:05:31.226377Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T17:05:47.777158Z",
     "start_time": "2019-09-16T17:05:47.770657Z"
    }
   },
   "outputs": [],
   "source": [
    "#borrowed function to iterate over docs and not kill RAM..\n",
    "#https://tmthyjames.github.io/2018/january/Analyzing-Rap-Lyrics-Using-Word-Vectors/\n",
    "\n",
    "class Sentences(object):\n",
    "    \n",
    "    def __init__(self, filename, column):\n",
    "        self.filename = filename\n",
    "        self.column = column\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_tokens(text):\n",
    "        \"\"\"Helper function for tokenizing data\"\"\"\n",
    "        #return [wnl.lemmatize(r.lower()) for r in text.split()]\n",
    "        return word_tokenize(text) #returns \n",
    "    \n",
    "    def __iter__(self):\n",
    "        reader = csv.DictReader(open(self.filename, 'r' ))\n",
    "        for row in reader:\n",
    "            words = self.get_tokens(row[self.column])\n",
    "            tags = ['%s|%s' % (row['Artist_name'], row['Song'])]\n",
    "            yield TaggedDocument(words=words, tags=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T17:05:51.602279Z",
     "start_time": "2019-09-16T17:05:51.385744Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'final_df.csv'\n",
    "sentences = Sentences(filename=filename, column='Lyrics') #column with lyrics\n",
    "# for song lookups\n",
    "orig_table = pd.read_csv(filename) # dont need artist song order in there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T17:13:17.357626Z",
     "start_time": "2019-09-16T17:13:17.339299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they', 'say', 'a', 'good', 'thing', 'won', 't', 'go', 'away', 'why', 'did', 'you', 'go', 'away', 'baby', 'you', 'had', 'a', 'role', 'to', 'play', 'why', 'were', 'you', 'led', 'astray', 'seems', 'all', 'love', 'that', 'you', 'were', 'given', 'didn', 't', 'have', 'a', 'second', 'for', 'me', 'but', 'a', 'second', 's', 'all', 'i', 'need', 'to', 'tell', 'you', 'the', 'truth', 'so', 'maybe', 'you', 'll', 'see', 'you', 'were', 'my', 'ally', 'ally', 'cause', 'were', 'there', 'was', 'you', 'there', 'would', 'be', 'me', 'you', 'were', 'my', 'ally', 'whether', 'wrong', 'or', 'right', 'you', 'were', 'by', 'my', 'side', 'and', 'i', 'still', 'want', 'you', 'want', 'you', 'and', 'i', 'don', 't', 'know', 'if', 'you', 'll', 'ever', 'really', 'feel', 'the', 'same', 'or', 'if', 'i', 'll', 'ever', 'forgive', 'the', 'fact', 'that', 'maybe', 'i', 'm', 'the', 'one', 'to', 'blame', 'cause', 'loving', 'you', 'were', 'given', 'didn', 't', 'have', 'a', 'second', 'to', 'breathe', 'but', 'a', 'second', 's', 'all', 'i', 'need', 'to', 'tell', 'you', 'the', 'truth', 'so', 'maybe', 'you', 'll', 'see', 'you', 'were', 'my', 'ally', 'ally', 'cause', 'were', 'there', 'was', 'you', 'there', 'would', 'be', 'me', 'you', 'were', 'my', 'ally', 'whether', 'wrong', 'or', 'right', 'you', 'were', 'by', 'my', 'side', 'and', 'i', 'still', 'want', 'you', 'want', 'you', 'want', 'you', 'want', 'you', 'come', 'back', 'to', 'me', 'come', 'back', 'to', 'me', 'come', 'back', 'to', 'me', 'come', 'back', 'to', 'me', 'come', 'back', 'to', 'me', 'come', 'back', 'to', 'me', 'come', 'back', 'to', 'me', 'come', 'back', 'to', 'me']\n"
     ]
    }
   ],
   "source": [
    "#Testin to make sure that teh tokenization function of the class works well\n",
    "print(sentences.get_tokens(orig_table.loc[0][\"Lyrics\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T13:49:42.214825Z",
     "start_time": "2019-09-13T13:49:42.208587Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(\n",
    "    alpha=0.05,\n",
    "    min_alpha=0.025,\n",
    "    workers=15, \n",
    "    min_count=10,\n",
    "    window=5,\n",
    "    vector_size=300,\n",
    "    epochs=30,\n",
    "    sample=0.001,\n",
    "    negative=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T13:54:59.041683Z",
     "start_time": "2019-09-13T13:49:46.660826Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielobennett/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(sentences) #build corpus of words by inserting sentences\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs) #train model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECKPOINT: Saving Doc2vec trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('alt_rnb_lyrics.doc2vec') #save model for flask app/later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T01:09:05.441925Z",
     "start_time": "2019-09-13T01:09:05.036210Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielobennett/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "#LOAD the model \n",
    "model = Doc2Vec.load('alt_rnb_lyrics.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the model NLP\n",
    "Using model.docvecs.most_similar will return the top n songs that have the most similar doc vectors.  \n",
    "NOTE: Returns the input song but we will make sure to exlcude that in the Flask app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T14:08:13.532252Z",
     "start_time": "2019-09-13T14:08:13.506802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The Weeknd|Call Out My Name', 0.9999998807907104),\n",
       " ('The Weeknd|Call Out My Name (A Cappella)', 0.9659292697906494),\n",
       " ('BANKS|Fall Over', 0.36231181025505066),\n",
       " ('Sinead Harnett|Ally', 0.3450368642807007),\n",
       " ('Rhye|Song for You', 0.3423432409763336),\n",
       " ('Autre Ne Veut|On & On', 0.331125408411026),\n",
       " ('Toro y Moi|Imprint After', 0.31505703926086426),\n",
       " ('Kevin Abstract|Runner (Original)', 0.3132482171058655),\n",
       " ('Kevin Abstract|Runner', 0.31315815448760986),\n",
       " ('blackbear|froze over', 0.30821770429611206),\n",
       " ('DEAN|instagram', 0.30410879850387573),\n",
       " ('Rhye|Shed Some Blood', 0.30292677879333496),\n",
       " ('Dawn Richard|Castles', 0.30236953496932983),\n",
       " ('Kid Cudi|Fuchsia Butterflies', 0.29958975315093994)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar([model.docvecs['The Weeknd|Call Out My Name']], topn=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing audio files\n",
    "Steps done to preporcess mp3 files for audio signal similarity:\n",
    "+ Create list of avaialable mp3s for modeling\n",
    "+ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T13:35:41.201251Z",
     "start_time": "2019-09-18T13:35:40.086741Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielobennett/anaconda3/lib/python3.7/site-packages/pydub/utils.py:179: RuntimeWarning: Couldn't find ffplay or avplay - defaulting to ffplay, but may not work\n",
      "  warn(\"Couldn't find ffplay or avplay - defaulting to ffplay, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import librosa \n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T04:52:03.729392Z",
     "start_time": "2019-09-09T04:52:03.723821Z"
    }
   },
   "source": [
    "### Working on conveting to spectrograms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T17:22:58.901267Z",
     "start_time": "2019-09-11T17:22:58.871658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of songs 138\n"
     ]
    }
   ],
   "source": [
    "#create list of mp3 files store in the audio_files folder \n",
    "mp3_files = !ls audio_files/*.mp3 \n",
    "print(\"number of songs\", len(mp3_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to spectros \n",
    "The create_save_spectrog function loads in a mp3 file, takes an n sec segement with a n sec delay (to take a segment that isnt from the start of the song) converts it to a spectrogram and saves it as a png file in the spectrograms subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T17:39:18.897609Z",
     "start_time": "2019-09-11T17:39:18.887619Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_save_spectrog(file_name,song_start=0,song_duration=20):\n",
    "    \"\"\"\n",
    "    Creates a spectrogram from the inputted mp3 file path and saves it in folder\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(file_name, offset=song_start, duration=song_duration)\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)#,fmax=8000\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(librosa.power_to_db(S ,ref=np.max))\n",
    "    plt.tight_layout()\n",
    "    temp_path = file_name.split('/')[1]\n",
    "    file_pref = temp_path.split('.mp3')[0]\n",
    "    output_f = \"spectrograms/\" + file_pref + \".png\"  #save in appropriate folder \n",
    "    plt.savefig(output_f, transparent=True, pad_inches=0.0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T17:43:00.204924Z",
     "start_time": "2019-09-11T17:39:22.416998Z"
    }
   },
   "outputs": [],
   "source": [
    "#add for loop to transform songs in with a 20 sec delay start and a 30 sec duration\n",
    "for songs in mp3_files:\n",
    "    '''\n",
    "    Find a way to go through many iterations of a song and split\n",
    "    '''\n",
    "    create_save_spectrog(songs, song_start=20, song_duration=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using VGG-16 Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T15:33:45.780532Z",
     "start_time": "2019-09-18T15:33:45.775379Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import cv2\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T18:59:16.433027Z",
     "start_time": "2019-09-16T18:59:15.723751Z"
    }
   },
   "outputs": [],
   "source": [
    "#Using VGG-16 trained on imagenet, set input waits\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))  # leaving out b/c optional, input_shape=(720, 288, 3)\n",
    "\n",
    "# Freeze convolutional layers: we do not want to re-train them \n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T18:59:16.526714Z",
     "start_time": "2019-09-16T18:59:16.462933Z"
    }
   },
   "outputs": [],
   "source": [
    "x = base_model.output \n",
    "x = Flatten(name='features')(x) # flatten from convolution tensor output, we will use this for feature extraction\n",
    "x = Dense(64,activation='relu')(x) #dense layer 2\n",
    "preds = Dense(5 ,activation='softmax')(x) #final layer with softmax activation / doesn't \n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T18:59:35.719270Z",
     "start_time": "2019-09-16T18:59:35.702788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "features (Flatten)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                1605696   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 16,320,709\n",
      "Trainable params: 1,606,021\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#checking the layers of the model \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T02:19:28.006265Z",
     "start_time": "2019-09-15T02:19:27.868586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/danielobennett/metis/work/projects/Project05\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T18:59:42.273756Z",
     "start_time": "2019-09-16T18:59:42.149596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 138 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "Train_dir = '/Users/danielobennett/metis/work/projects/Project05/Train_dir'\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        Train_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-16T19:22:06.776808Z",
     "start_time": "2019-09-16T18:59:47.380825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/danielobennett/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "7/7 [==============================] - 73s 10s/step - loss: 0.7004 - acc: 0.7241\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 69s 10s/step - loss: 0.5349 - acc: 0.8000\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 67s 10s/step - loss: 0.4835 - acc: 0.8000\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 66s 9s/step - loss: 0.4620 - acc: 0.8014\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 67s 10s/step - loss: 0.4429 - acc: 0.8016\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 66s 9s/step - loss: 0.4224 - acc: 0.8000\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 66s 9s/step - loss: 0.4072 - acc: 0.8029\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 67s 10s/step - loss: 0.3924 - acc: 0.8086\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 66s 9s/step - loss: 0.3753 - acc: 0.8157\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 66s 9s/step - loss: 0.3531 - acc: 0.8249\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 67s 10s/step - loss: 0.3468 - acc: 0.8305\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 67s 10s/step - loss: 0.3329 - acc: 0.8376\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 67s 10s/step - loss: 0.3136 - acc: 0.8562\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 67s 10s/step - loss: 0.2975 - acc: 0.8660\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 66s 9s/step - loss: 0.2801 - acc: 0.8844\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 67s 10s/step - loss: 0.2848 - acc: 0.8825\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 67s 10s/step - loss: 0.2711 - acc: 0.8851\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 66s 9s/step - loss: 0.2432 - acc: 0.9069\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 67s 10s/step - loss: 0.2339 - acc: 0.9173\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 66s 9s/step - loss: 0.2204 - acc: 0.9303\n"
     ]
    }
   ],
   "source": [
    "#compile and run model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=7,\n",
    "      epochs=20,\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECKPOINT: Saving cnn model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T15:44:33.588259Z",
     "start_time": "2019-09-18T15:44:31.794711Z"
    }
   },
   "outputs": [],
   "source": [
    "#save the model so there is no need to rerun\n",
    "#model.save('cnn_music.h5')\n",
    "from keras.models import load_model\n",
    "model = load_model('cnn_music.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T22:12:56.133354Z",
     "start_time": "2019-09-17T22:12:56.121942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "features (Flatten)           (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                1605696   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 16,320,709\n",
      "Trainable params: 1,606,021\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#check summary of model once again\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T15:44:41.994892Z",
     "start_time": "2019-09-18T15:44:41.973940Z"
    }
   },
   "outputs": [],
   "source": [
    "#Extract features from the flattened layer\n",
    "feature_extractor = Model(inputs=model.input, outputs=model.get_layer('features').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T20:24:15.537934Z",
     "start_time": "2019-09-18T20:24:14.963181Z"
    }
   },
   "outputs": [],
   "source": [
    "#save feature model since it will be smaller in size for flask app \n",
    "feature_extractor.save('model_predict.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T15:44:43.945220Z",
     "start_time": "2019-09-18T15:44:43.895947Z"
    }
   },
   "outputs": [],
   "source": [
    "#get the paths of the spectrograms created above\n",
    "spectros = !ls spectrograms/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:46:14.227722Z",
     "start_time": "2019-09-18T17:45:16.526121Z"
    }
   },
   "outputs": [],
   "source": [
    "features_vectors = {}\n",
    "\n",
    "for item in spectros:\n",
    "    img = cv2.imread(item)\n",
    "    img = cv2.resize(img,(224,224))\n",
    "    img = np.reshape(img,[1,224,224,3])\n",
    "    img_vector = feature_extractor.predict(img)\n",
    "    song = item.split('/')[1]\n",
    "    features_vectors[song] = img_vector.tolist() #creates all as a python list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T19:47:38.153829Z",
     "start_time": "2019-09-18T19:47:37.051984Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(features_vectors, orient='index')\n",
    "df.reset_index(inplace=True)\n",
    "df.columns = ['song', 'vectors']\n",
    "df.to_pickle('flask_df.pkl') #need to pickle so that the vector column is a series of lists and not strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T18:13:50.575677Z",
     "start_time": "2019-09-18T18:13:50.560283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    \\n    plt.figure(figsize=[8,8])\\n    im = mpimg.imread(new_path)\\n    plt.imshow(im)\\n    plt.title('Input')\\n    plt.axis('off');\\n    \\n    rec1 = top_recommendations.index[1]\\n    rec2 = top_recommendations.index[2]\\n    rec3 = top_recommendations.index[3]\\n    rec4 = top_recommendations.index[4]\\n    rec5 = top_recommendations.index[5]\\n    rec6 = top_recommendations.index[6]\\n\""
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract vector for new item, need to convert to spectrogram before hand\n",
    "def extract_vector(new_path):\n",
    "    y, sr = librosa.load(new_path, offset=20, duration=30)\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)#,fmax=8000\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(librosa.power_to_db(S ,ref=np.max))\n",
    "    plt.tight_layout()\n",
    "    temp_path = new_path.split('/')[1]\n",
    "    file_pref = temp_path.split('.mp3')[0]\n",
    "    output_f = \"test_songs/\" + file_pref + \".png\"  #save in appropriate folder \n",
    "    plt.savefig(output_f, transparent=True, pad_inches=0.0)\n",
    "    plt.close()\n",
    "        #save the file and open with opencv######## \n",
    "    img = cv2.imread(output_f) #no need to read in \n",
    "    img = cv2.resize(img,(224,224))\n",
    "    img = np.reshape(img,[1,224,224,3])\n",
    "    img_vector = feature_extractor.predict(img)\n",
    "    return img_vector\n",
    "\n",
    "#calculate cosign distance\n",
    "def cosine_similarity(row, recommendation_vector):\n",
    "    distance = cosine(row[\"vectors\"], recommendation_vector)\n",
    "    return distance\n",
    "    \n",
    "#get recos from model \n",
    "def plot_recommendations(new_path, recommendation_name):\n",
    "    rec_vector = extract_vector(new_path)\n",
    "    df[recommendation_name] = df.apply(cosine_similarity, axis=1, recommendation_vector = rec_vector)\n",
    "    top_recommendations = df.sort_values(recommendation_name, ascending=False)#.head(10)\n",
    "    return top_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T05:04:10.408846Z",
     "start_time": "2019-09-19T05:04:07.287899Z"
    }
   },
   "outputs": [],
   "source": [
    "feed1 = 'test_songs/Ibeyi-River.mp3' #name of input mp3 to use for recommendation system\n",
    "try1 = plot_recommendations(feed1, recommendation_name = \"recommendation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T05:04:20.722149Z",
     "start_time": "2019-09-19T05:04:20.640598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>vectors</th>\n",
       "      <th>recommendation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>R.LUM.R_-_Suddenly_altered.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.511550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>The_Weeknd_-_Tears_In_The_Rain.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.415480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Jhene_Aiko-_Wading_(Souled_Out).png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.407254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ibeyi_ft._Kamasi_Washington_-_Deathless.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.389001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>The_Weeknd_-_Devil_May_Cry.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.371913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Childish_Gambino_-_All_The_Shine.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.367026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Sinead_Harnett_-_Body_Acoustic.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.363244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Majid_Jordan_-_King_City.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.361324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Mac_Ayres_-_Calvin's_Joint.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.359122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>The_Weeknd_-_I_Was_Never_There_(Official_Audio...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.348378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Childish_Gambino-_So_Fly.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.347232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Jorja_Smith_-_Something_In_The_Way.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.340009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>dvsn_-_Nuh_TimeTek_Time_(Official_Audio).png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.337110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Childish_Gambino_-_Flight_of_the_Navigator.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.331315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Childish_Gambino_-_Heartbeat.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.328948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Khalid_-_Coaster.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.315456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arlissa_-_Every_Time_I_Breathe.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.309532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>The_Weeknd_-_The_Birds_Pt._2.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.298091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>The_Weeknd_-_Privilege_(Official_Audio).png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.295708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>The_Weeknd_-_Where_You_Belong.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.287136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>The_Weeknd_-_Outside.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.286771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Arlissa_-_Praying_for_Love.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.285428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>R.LUM.R_-_Frustrated_Acoustic.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.284417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Jhene_Aiko_-_Oblivion_(Creation).png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.283332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Childish_Gambino_-_You_See_Me.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.278922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Mac_Ayers_-_Summertime.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.277522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>The_Weeknd_-_The_Knowing.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.275454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>The_Weeknd_-_The_Fall.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.269576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>dvsn_-_mood_(Official_Audio).png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.267301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>dvsn_-_Cant_Wait_(Official_Audio).png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.267017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Kehlani_-_Love_Language.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.175928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>dvsn_-_Conversations_In_A_Diner_(Official_Audi...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.175850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>R.LUM.R_-_Be_Honest.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.173843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Jorja_Smith_-_Imperfect_Circle.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.173205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Jhene_Aiko_-_Never_Call_Me.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.171811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Kiana_Lede_-_I_Choose_You_Acoustic.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.170490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Kehlani_I_Wanna_Be_[Official_Audio].png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.169974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arlissa_-_Getting_Older.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.169822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Mahalia_-_Hide_Out.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.168070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Kehlani_-_Wanted_[Official_Audio].png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.168020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Jhene_Aiko_-_Moments.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.167233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Kehlani_In_My_Feelings_[Official_Audio].png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.166720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Khalid_-_Paradise.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.166248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>R.LUM.R_-_Cold.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.164924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Jhene_Aiko_-_Living_Room_Flow.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.164835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Jhene_Aiko_-_The_Pressure.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.163805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Mahalia_-_Karma.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.162829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>The_Weeknd_-_Comin_Out_Strong.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.161131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>The_Weeknd_-_Rescue_You.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.160332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Kiana_Lede_-_Can_I.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.160079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>The_Weeknd_-_Enemy.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.157216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Jorja_Smith_-_A_Prince_ft._Maverick_Sabre.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.156674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>The_Weeknd_Valerie_Lyrics.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.155257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Jhene_Aiko-_Lyin_King_(Souled_Out).png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.147189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>The_Weeknd_-_Nomads.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.146476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Sinead_Harnett_-_Lessons.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.146427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Sinead_Harnett_-_So_Solo.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.142734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Mac_Ayers_-_Walking_Home.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.133776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>The_Weeknd_-_Initiation.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.132004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Kiana_Lede_-_Shame.png</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.130874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  song  \\\n",
       "84                      R.LUM.R_-_Suddenly_altered.png   \n",
       "114                 The_Weeknd_-_Tears_In_The_Rain.png   \n",
       "28                 Jhene_Aiko-_Wading_(Souled_Out).png   \n",
       "23         Ibeyi_ft._Kamasi_Washington_-_Deathless.png   \n",
       "95                      The_Weeknd_-_Devil_May_Cry.png   \n",
       "10                Childish_Gambino_-_All_The_Shine.png   \n",
       "87                  Sinead_Harnett_-_Body_Acoustic.png   \n",
       "74                        Majid_Jordan_-_King_City.png   \n",
       "70                      Mac_Ayres_-_Calvin's_Joint.png   \n",
       "100  The_Weeknd_-_I_Was_Never_There_(Official_Audio...   \n",
       "9                         Childish_Gambino-_So_Fly.png   \n",
       "44              Jorja_Smith_-_Something_In_The_Way.png   \n",
       "133       dvsn_-_Nuh_TimeTek_Time_(Official_Audio).png   \n",
       "13      Childish_Gambino_-_Flight_of_the_Navigator.png   \n",
       "14                    Childish_Gambino_-_Heartbeat.png   \n",
       "55                                Khalid_-_Coaster.png   \n",
       "1                   Arlissa_-_Every_Time_I_Breathe.png   \n",
       "116                   The_Weeknd_-_The_Birds_Pt._2.png   \n",
       "110        The_Weeknd_-_Privilege_(Official_Audio).png   \n",
       "124                  The_Weeknd_-_Where_You_Belong.png   \n",
       "109                           The_Weeknd_-_Outside.png   \n",
       "5                       Arlissa_-_Praying_for_Love.png   \n",
       "81                   R.LUM.R_-_Frustrated_Acoustic.png   \n",
       "35                Jhene_Aiko_-_Oblivion_(Creation).png   \n",
       "19                   Childish_Gambino_-_You_See_Me.png   \n",
       "68                          Mac_Ayers_-_Summertime.png   \n",
       "118                       The_Weeknd_-_The_Knowing.png   \n",
       "117                          The_Weeknd_-_The_Fall.png   \n",
       "137                   dvsn_-_mood_(Official_Audio).png   \n",
       "129              dvsn_-_Cant_Wait_(Official_Audio).png   \n",
       "..                                                 ...   \n",
       "48                         Kehlani_-_Love_Language.png   \n",
       "130  dvsn_-_Conversations_In_A_Diner_(Official_Audi...   \n",
       "79                             R.LUM.R_-_Be_Honest.png   \n",
       "43                  Jorja_Smith_-_Imperfect_Circle.png   \n",
       "34                      Jhene_Aiko_-_Never_Call_Me.png   \n",
       "62              Kiana_Lede_-_I_Choose_You_Acoustic.png   \n",
       "51             Kehlani_I_Wanna_Be_[Official_Audio].png   \n",
       "2                          Arlissa_-_Getting_Older.png   \n",
       "72                              Mahalia_-_Hide_Out.png   \n",
       "50               Kehlani_-_Wanted_[Official_Audio].png   \n",
       "33                            Jhene_Aiko_-_Moments.png   \n",
       "52         Kehlani_In_My_Feelings_[Official_Audio].png   \n",
       "57                               Khalid_-_Paradise.png   \n",
       "80                                  R.LUM.R_-_Cold.png   \n",
       "32                   Jhene_Aiko_-_Living_Room_Flow.png   \n",
       "36                       Jhene_Aiko_-_The_Pressure.png   \n",
       "73                                 Mahalia_-_Karma.png   \n",
       "93                   The_Weeknd_-_Comin_Out_Strong.png   \n",
       "111                        The_Weeknd_-_Rescue_You.png   \n",
       "60                              Kiana_Lede_-_Can_I.png   \n",
       "97                              The_Weeknd_-_Enemy.png   \n",
       "40       Jorja_Smith_-_A_Prince_ft._Maverick_Sabre.png   \n",
       "126                      The_Weeknd_Valerie_Lyrics.png   \n",
       "26              Jhene_Aiko-_Lyin_King_(Souled_Out).png   \n",
       "107                            The_Weeknd_-_Nomads.png   \n",
       "89                        Sinead_Harnett_-_Lessons.png   \n",
       "90                        Sinead_Harnett_-_So_Solo.png   \n",
       "69                        Mac_Ayers_-_Walking_Home.png   \n",
       "101                        The_Weeknd_-_Initiation.png   \n",
       "64                              Kiana_Lede_-_Shame.png   \n",
       "\n",
       "                                               vectors  recommendation  \n",
       "84   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.511550  \n",
       "114  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.415480  \n",
       "28   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.407254  \n",
       "23   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.389001  \n",
       "95   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.371913  \n",
       "10   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.367026  \n",
       "87   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.363244  \n",
       "74   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.361324  \n",
       "70   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.359122  \n",
       "100  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.348378  \n",
       "9    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.347232  \n",
       "44   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.340009  \n",
       "133  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.337110  \n",
       "13   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.331315  \n",
       "14   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.328948  \n",
       "55   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.315456  \n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.309532  \n",
       "116  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.298091  \n",
       "110  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.295708  \n",
       "124  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.287136  \n",
       "109  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.286771  \n",
       "5    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.285428  \n",
       "81   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.284417  \n",
       "35   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.283332  \n",
       "19   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.278922  \n",
       "68   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.277522  \n",
       "118  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.275454  \n",
       "117  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.269576  \n",
       "137  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.267301  \n",
       "129  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.267017  \n",
       "..                                                 ...             ...  \n",
       "48   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.175928  \n",
       "130  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.175850  \n",
       "79   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.173843  \n",
       "43   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.173205  \n",
       "34   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.171811  \n",
       "62   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.170490  \n",
       "51   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.169974  \n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.169822  \n",
       "72   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.168070  \n",
       "50   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.168020  \n",
       "33   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.167233  \n",
       "52   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.166720  \n",
       "57   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.166248  \n",
       "80   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.164924  \n",
       "32   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.164835  \n",
       "36   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.163805  \n",
       "73   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.162829  \n",
       "93   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.161131  \n",
       "111  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.160332  \n",
       "60   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.160079  \n",
       "97   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.157216  \n",
       "40   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.156674  \n",
       "126  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.155257  \n",
       "26   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.147189  \n",
       "107  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.146476  \n",
       "89   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.146427  \n",
       "90   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.142734  \n",
       "69   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.133776  \n",
       "101  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.132004  \n",
       "64   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        0.130874  \n",
       "\n",
       "[138 rows x 3 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try1 #current one : #treat me like im yours 10 sec max > arlissa whats it gonna be 10 sec max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Notebook :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1081px",
    "right": "20px",
    "top": "104px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
